{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "randint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "data = pd.read_excel(os.path.join('D:\\毕设code\\\\news_data_from2016to2021','2021','ER_NewsInfo.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[100:150,'Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[60,'NewsContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DeclareDate'] = data['DeclareDate'].apply(lambda x : datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data[data['DeclareDate'] == specific_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.reset_index(drop=True).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_trans(date):\n",
    "    \"\"\"时间格式转换\n",
    "\n",
    "    Args:\n",
    "        date (str): 字符串'20210112'这种\n",
    "\n",
    "    Returns:\n",
    "        [time]: 规范化之后的时间格式\n",
    "    \"\"\"\n",
    "    date = pd.to_datetime(date)\n",
    "    return date.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_date = date_trans('20210601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "time = ['2021','12','26']\n",
    "a  = datetime.date(int(time[0]),int(time[1]),int(time[2]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2use = data['NewsContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt','w') as f:\n",
    "    for idx,content in enumerate(data2use):\n",
    "        if idx <= 100:\n",
    "            f.write(str(idx) + ' ' + str(content),)\n",
    "        else:\n",
    "            break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhon  import hanzi\n",
    "import string\n",
    "string.punctuation,type(hanzi.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.to_datetime('20211212')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('news_data_from2016to2021\\companies\\深交所A股列表_主板.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[2,'A股简称']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "with open('D:\\chrome\\Listed-company-news-crawl-and-text-analysis-master\\src\\Leorio\\chnstopwords.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让文本只保留汉字\n",
    "def is_chinese(uchar):\n",
    "    \"\"\"判断是否是汉字\n",
    "\n",
    "    Args:\n",
    "        uchar (str): 汉字字符串\n",
    "\n",
    "    Returns:\n",
    "        [bool]: 是或者不是\n",
    "    \"\"\"\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def format_str(content):\n",
    "    \"\"\"返回句子\n",
    "\n",
    "    Args:\n",
    "        content (str): 要处理的中文句子\n",
    "\n",
    "    Returns:\n",
    "        [str]: 取掉标点符号之后的句子\n",
    "    \"\"\"\n",
    "    content_str = ''\n",
    "    for i in content:\n",
    "        if is_chinese(i):\n",
    "            content_str = content_str + i\n",
    "    return content_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = '《中华人民共和国刑法》的颁布对于国家生活产生了重大影响'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = format_str(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "seg_list = jieba.cut(new,use_paddle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "seg_list = jieba.cut(new,cut_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install paddlepaddle-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('D:\\毕设code\\\\news_data_from2016to2021\\companies\\\\ts_stock_price\\\\000001.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[len(data)-1,'trade_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.listdir('news_data_from2016to2021\\companies\\\\final_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_left_company(src_path):\n",
    "    \"\"\"返回文件列表\n",
    "\n",
    "    Args:\n",
    "        src_path (str): 文件父目录\n",
    "\n",
    "    Returns:\n",
    "        [list]: 文件列表\n",
    "    \"\"\"\n",
    "    return os.listdir(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_company_data_info(src_path):\n",
    "    \"\"\"返回数据的简单统计特征\n",
    "\n",
    "    Returns:\n",
    "        [dataframe]: 数据的统计特征,包括极值,方差等。\n",
    "    \"\"\"\n",
    "    #随机确定公司\n",
    "    company_name_code = new_left_company(src_path)\n",
    "    company_code = choice(company_name_code)\n",
    "    #读数据\n",
    "    company_hist = pd.read_csv(src_path + '\\\\' + company_code,\n",
    "                               encoding='gbk')\n",
    "    return company_hist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_company_data_info('news_data_from2016to2021\\companies\\\\final_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "def missing_fulfill(trade_date, company_price):\n",
    "    \"\"\"缺失值补充\n",
    "\n",
    "    Args:\n",
    "        trade_date (dataframe): 交易日数据的dataframe\n",
    "        company_price (dataframe): 公司数据的dataframe\n",
    "\n",
    "    Returns:\n",
    "        dataframe: 补充缺失值之后的数据\n",
    "    \"\"\"\n",
    "    total_num = len(trade_date)\n",
    "    company_data_num = len(company_price)\n",
    "    if total_num == company_data_num:\n",
    "        return company_price\n",
    "    else:\n",
    "        data_need = total_num - company_data_num\n",
    "        print(data_need)\n",
    "        if data_need > 0:\n",
    "            flag = True\n",
    "            temp = 0  #记录哪一行有问题\n",
    "            times = 0  #记录修改了多少次\n",
    "            while flag:\n",
    "                for i in range(temp, len(company_price)):\n",
    "                    if company_price.loc[i, 'trade_date'] == trade_date.loc[\n",
    "                            total_num-1 - i, 'cal_date']:\n",
    "                        continue  #说明在trade_date.loc[-i,'cal_date']日期有数据\n",
    "                    else:\n",
    "                        value = [trade_date.loc[total_num - i, 'cal_date']\n",
    "                                 ] + [np.nan] * (len(company_price.columns) - 1)\n",
    "                        df_add = dict(zip(company_price.columns, value))\n",
    "                        df_add = pd.DataFrame([df_add])\n",
    "                        company_price = insert(company_price, i, df_add)\n",
    "                        temp = i\n",
    "                        times += 1\n",
    "                        break\n",
    "                if times == data_need:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    flag = True\n",
    "            #这里采取线条插值，阶数定位3\n",
    "            company_price = company_price.interpolate(method = 'spline', order = 3)\n",
    "            print('trade days:' + str(total_num))\n",
    "            print('company data number:'+ str(len(company_price)))\n",
    "            return company_price\n",
    "        else:\n",
    "            print('公司数据量更多，请检查具体数据内容！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def insert(df, i, df_add):\n",
    "    \"\"\"在指定行插入数据\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): 要操作的数据\n",
    "        i (int): 行数\n",
    "        df_add (dataframe): 要插入的数据\n",
    "\n",
    "    Returns:\n",
    "        [dataframe]: 插入之后的dataframe\n",
    "    \"\"\"\n",
    "    # 指定第i行插入一行数据\n",
    "    df1 = df.iloc[:i, :]\n",
    "    df2 = df.iloc[i:, :]\n",
    "    df_new = pd.concat([df1, df_add, df2], ignore_index=True)\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trade_day(path):\n",
    "    \"\"\"获取交易日数据,注意应该是2017-01-01 到 2021-12-31之间，之后的标签也是基于这中间的数据进行的\n",
    "\n",
    "    Args:\n",
    "        path (str): 交易日数据\n",
    "\n",
    "    Returns:\n",
    "        [dataframe]: 交易日的dataframe,只有两列\n",
    "    \"\"\"\n",
    "    date = pd.read_csv(path, encoding='gbk')\n",
    "    date = date[date['cal_date'] >= 20170101].reset_index(drop=True)\n",
    "    date = date[date['is_open'] == 1]\n",
    "    return date.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_day = 'news_data_from2016to2021\\companies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_final = 'D:\\毕设code\\\\news_data_from2016to2021\\companies\\\\final_use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = get_trade_day(trade_day + '\\\\' + 'trade_cal_clean.csv')\n",
    "company_hist = pd.read_csv(os.path.join(company_final, '002137' + '.csv'),\n",
    "                           encoding='gbk').iloc[:, 2:-1]\n",
    "company_hist = company_hist[\n",
    "    company_hist['trade_date'] >= 20170101].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_hist = missing_fulfill(date, company_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([1, 2, 3, 4, 5])\n",
    "reverse_df = df.reindex(index=df.index[::-1])\n",
    "#reverse_df = df.iloc[::-1]\n",
    "reverse_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 20211221\n",
    "str(a)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from price_preprocess import get_company_name_code\n",
    "triple = get_company_name_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in triple:\n",
    "    company_name, company_code, company_short = elem\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name, company_code, company_short "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '20210712'\n",
    "year = '2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_date_news(date, year):\n",
    "    \"\"\"获取指定日期的全部新闻\n",
    "\n",
    "    Args:\n",
    "        date (date): 需要返回数据的日期\n",
    "        year (str): 用哪一年的数据\n",
    "\n",
    "    Returns:\n",
    "        [dataframe]: 指定日期的全部资讯的dataframe\n",
    "    \"\"\"\n",
    "    date = date_trans(date)\n",
    "    news_data = get_news_data(year)\n",
    "    news_data['DeclareDate'] = news_data['DeclareDate'].apply(\n",
    "        lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d'\n",
    "                                                                     ))\n",
    "    return news_data[news_data['DeclareDate'] == date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['DeclareDate', 'Classify', 'Title', 'NewsContent', 'Symbol']\n",
    "news_path = 'D:\\毕设code\\\\news_data_from2016to2021'\n",
    "file_name = 'ER_NewsInfo.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(year):\n",
    "    \"\"\"获取指定年份的新闻资讯数据\n",
    "\n",
    "    Args:\n",
    "        year (int): 年份\n",
    "\n",
    "    Returns:\n",
    "        [dataframe]: 新闻资讯数据的dataframe,列名也修改了，注意！\n",
    "    \"\"\"\n",
    "    news_name = os.path.join(news_path, str(year), file_name)\n",
    "    news_data = pd.read_excel(news_name)  #获取新闻资讯数据\n",
    "    return news_data[columns_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_trans(date):\n",
    "    \"\"\"时间格式转换\n",
    "\n",
    "    Args:\n",
    "        date (str): 字符串'20210112'这种\n",
    "\n",
    "    Returns:\n",
    "        [time]: 规范化之后的时间格式,2021-01-12\n",
    "    \"\"\"\n",
    "    date = pd.to_datetime(date)\n",
    "    return date.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取指定日期全部新闻\n",
    "specific_date_news = get_specific_date_news(date, year)\n",
    "#specific_date_news = specific_date_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_l = specific_date_news.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_l[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_date_news.index.tolist().pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_date_news = specific_date_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_date_news.loc[:,'Symbol'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(specific_date_news)):\n",
    "    if specific_date_news.loc[idx,'Title'].find(company_name) != -1 or \\\n",
    "        specific_date_news.loc[idx,'NewsContent'].find(company_name) != -1 or \\\n",
    "        specific_date_news.loc[idx,'Title'].find(company_code) != -1 or \\\n",
    "        specific_date_news.loc[idx,'NewsContent'].find(company_code) != -1 or \\\n",
    "        specific_date_news.loc[idx,'Title'].find(company_short) != -1 or \\\n",
    "        specific_date_news.loc[idx,'NewsContent'].find(company_short) != -1 or \\\n",
    "        company_code in [str(code).zfill(6) for code in specific_date_news.loc[idx,'Symbol']]:\n",
    "        print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thread\n",
    "import time\n",
    " \n",
    "# 为线程定义一个函数\n",
    "def print_time( threadName, delay):\n",
    "   count = 0\n",
    "   while count < 5:\n",
    "      time.sleep(delay)\n",
    "      count += 1\n",
    "      print ('%s: %s' % ( threadName, time.ctime(time.time()) ))\n",
    " \n",
    "# 创建两个线程\n",
    "try:\n",
    "   thread.start_new_thread( print_time, (\"Thread-1\", 2, ) )\n",
    "   thread.start_new_thread( print_time, (\"Thread-2\", 4, ) )\n",
    "except:\n",
    "   print ('Error: unable to start thread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(99,100):\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dicts = []\n",
    "with open('D:\\毕设code\\\\news_data_from2016to2021\\\\2020\\\\2020_data.json','rb') as f:\n",
    "    dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "list(jieba.cut('中华人民共和国刑法在人民生活中发挥了巨大作用',use_paddle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "nodes =['a','b','c','d','e','f','j','l']\n",
    "Graph = nx.Graph()\n",
    "Graph.add_nodes_from(nodes)\n",
    "nx.draw(Graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('..')\n",
    "from preprocessing.price_preprocess import new_left_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_path = 'D:\\\\毕设code\\\\news_data_from2016to2021'\n",
    "Year = ['2017'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_text_remove(text):\n",
    "    \"\"\"对文本列表删除长度小于3的内容\n",
    "\n",
    "    Args:\n",
    "        text (list): 不同篇资讯的嵌套列表\n",
    "\n",
    "    Returns:\n",
    "        list: 去除之后的列表，分两种一种是区分文章，一种是合并文章，这里先采取合并文章的办法\n",
    "    \"\"\"\n",
    "    #事实上有的时候，资讯不止一条，所以需要遍历一下\n",
    "    new_text = []\n",
    "    #前途是非空,并且不是嵌套了一个空的列表，len([[]])= 1\n",
    "    if text and text != [[]] * len(text):\n",
    "        #print('begining to remove short length sentence...')\n",
    "        for index in range(len(text)):\n",
    "            #还是得非空\n",
    "            if text[index]:\n",
    "                text_left = []\n",
    "                for elem in text[index]:\n",
    "                    if len(elem) > 3:  #排除类似'万亿元'这种的内容\n",
    "                        text_left.append(elem)\n",
    "                #这里先以将所有有的资讯全部合并到一起,变成一篇文章，之后能够处理相应的数据格式了再说可以改\n",
    "                #next_text.append(text_left)\n",
    "                new_text += text_left\n",
    "            else:\n",
    "                continue\n",
    "        return new_text\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_fullfil(Year, Data_path, company_list):\n",
    "\n",
    "    #得到数据路径\n",
    "    func = lambda x: os.path.join(Data_path, x, x + '_data.json')\n",
    "    news_path = list(map(func, Year))\n",
    "    #先把所有咨询读取，然后放在一个列表中\n",
    "    news_list = []\n",
    "    for news in news_path:\n",
    "        print('Reading data from: ' + news)\n",
    "        with open(news, 'r',encoding='utf-8') as f:\n",
    "            content_ = json.load(f)\n",
    "            news_list.append(content_)\n",
    "    #最后的数据保存为列表来返回\n",
    "    global all_data\n",
    "    all_data = []\n",
    "    #有多少家公司\n",
    "    company_num = len(company_list)\n",
    "    print('number of company：', company_num)\n",
    "    print('Begining to fullfil missing news for some day...')\n",
    "    #开始处理\n",
    "    for i in range(len(news_list)):\n",
    "        news = news_list[i]\n",
    "        print('preprocessing ' + news_path[i])\n",
    "        #存储时间日期\n",
    "        global date\n",
    "        date = []\n",
    "        #存储是否是交易日\n",
    "        global is_open\n",
    "        is_open = []\n",
    "        #存储公司数据\n",
    "        company_data = []\n",
    "        for elem in news:\n",
    "            # key, value = elem.items() 是不对的，会报错\n",
    "            for key, value in elem.items():\n",
    "                date.append(key)\n",
    "                is_open.append(int(value['is_open']))  #注意字典里面是字符串\n",
    "                company_data.append(value['company'])\n",
    "        #将数据翻转过来，便于操作\n",
    "        date = date[::-1]\n",
    "        is_open = is_open[::-1]\n",
    "        company_data = company_data[::-1]\n",
    "        print(\n",
    "            'finished dict_data classification and begining to fullfil missing data...'\n",
    "        )\n",
    "        #记录一下有多少天\n",
    "        total_days = len(date)\n",
    "        #第一段处理过程\n",
    "        #先把数据处理为需要的格式\n",
    "        for j in range(total_days):\n",
    "            #先把所有的数据都处理一下\n",
    "            for company in company_list:  #注意company_list是字符串的列表\n",
    "                com_news = company_data[j].get(company)\n",
    "                if com_news != None:\n",
    "                    title = com_news['title']  #这个先不动，事实上篇幅也不长，所以先不动\n",
    "                    news_content = short_text_remove(\n",
    "                        com_news['content'])  #直接进行一波处理\n",
    "                    #接下来这一部分是为了数据格式\n",
    "                    dict_data = OrderedDict()\n",
    "                    dict_data['date'] = date[j]\n",
    "                    #dict_data['is_open'] = str(is_open[j]) 之后没什么用\n",
    "                    dict_data['company'] = company\n",
    "                    dict_data['title'] = title\n",
    "                    dict_data['news'] = news_content\n",
    "                    dict_data['last_news_period'] = 0  #目前先默认为0\n",
    "                    dict_data['last_title_period'] = 0\n",
    "                    all_data.append(dict_data)\n",
    "                else:\n",
    "                    print(company + ' data is not found and need debug...')\n",
    "                    break\n",
    "        print('finished the format of data processing...')\n",
    "        print('output one piece of data...')\n",
    "        for keys,values in all_data[random.randint(1, 10000)].items():\n",
    "                print(keys,values)\n",
    "\n",
    "        #开始另一段处理过程\n",
    "        #all_data = no_tradeday_fullfil(all_data, is_open, company_num)\n",
    "\n",
    "        with open(os.path.join('D:\\\\毕设code\\\\data', Year[i] + '.json'), 'wb') as f:\n",
    "            json_final_data = json.dumps(\n",
    "                all_data,indent=4, ensure_ascii=False).encode(encoding='utf-8')\n",
    "            f.write(json_final_data)\n",
    "            print(f'完成{Year[i]}年文件写入')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_num = 1135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_trade(index_,is_open):\n",
    "    \"\"\"得到相互补充数据的相邻几天\n",
    "\n",
    "    Args:\n",
    "        index (int): 非交易日的第一个下标\n",
    "        is_open (list): 交易日的列表\n",
    "\n",
    "    Returns:\n",
    "        list: 非交易日和最近的一个交易日的下标列表\n",
    "    \"\"\"\n",
    "    index_list = []\n",
    "    while index_ < len(is_open) and (not is_open[index_]):\n",
    "        index_list.append(index_)\n",
    "        index_ += 1\n",
    "    if index_ < len(is_open):\n",
    "        index_list.append(index_)\n",
    "    return index_list,is_open[index_list[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_compliment_not(index_new,all_data):\n",
    "    \"\"\"对交易日数据进行补充,主要是第一天是非交易日\n",
    "\n",
    "    Args:\n",
    "        index_new (zip): 下标对组成的zip对象\n",
    "        all_data (list): 数据列表\n",
    "\n",
    "    Returns:\n",
    "        list: 处理后的列表\n",
    "    \"\"\"\n",
    "    data_use = []\n",
    "    index_pair = []\n",
    "    for idx1,idx2 in index_new:\n",
    "        index_pair.append((idx1,idx2))\n",
    "        data_use.append(all_data[idx1:idx2])\n",
    "    #补数据\n",
    "    for data in data_use[:-1]:\n",
    "        for index,company_ in enumerate(data):\n",
    "            data_use[-1][index]['title'] += company_['title']\n",
    "            data_use[-1][index]['news'] += company_['news']\n",
    "    #数据替换\n",
    "    all_data[index_pair[-1][0]:index_pair[-1][1]] = data_use[-1]\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_compliment_yes(index_new,all_data):\n",
    "    \"\"\"对交易日数据进行补充,主要是第一天是交易日\n",
    "\n",
    "    Args:\n",
    "        index_new (zip): 下标对组成的zip对象\n",
    "        all_data (list): 数据列表\n",
    "\n",
    "    Returns:\n",
    "        list: 处理后的列表\n",
    "    \"\"\"\n",
    "    #先读数据\n",
    "    data_use = []\n",
    "    index_pair = []\n",
    "    for idx1,idx2 in index_new:\n",
    "        index_pair.append((idx1,idx2))\n",
    "        data_use.append(all_data[idx1:idx2])\n",
    "    #补数据\n",
    "    time_func = lambda x:datetime.strptime(x,'%Y-%m-%d')\n",
    "    for index,data in enumerate(data_use[0]):\n",
    "        #判断某天是否有标题数据\n",
    "        if not data['title']:\n",
    "            for compliment_data in data_use[1:]:\n",
    "                compliment_data_ = compliment_data[index]\n",
    "                if not compliment_data_['title'] and compliment_data_['title'] != [[]]*len(compliment_data_['title']):\n",
    "                    data['title'] += compliment_data_['title']\n",
    "                    data['last_title_period'] = (time_func(data['date'])-time_func(compliment_data_['date'])).days\n",
    "            print(f'complimented news title of {index}_th company...')\n",
    "        #判断某天是否有正文数据\n",
    "\n",
    "        if not data['news']:\n",
    "            for compliment_data in data_use[1:]:\n",
    "                compliment_data_ = compliment_data[index]\n",
    "                if not compliment_data_['news'] :\n",
    "                    data['news'] += compliment_data_['news']\n",
    "                    data['last_news_period'] = (time_func(data['date'])-time_func(compliment_data_['date'])).days\n",
    "            print(f'complimented news content of {index}_th company...')\n",
    "    #数据替换\n",
    "    all_data[index_pair[0][0]:index_pair[0][1]] = data_use[0]\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_num = 1135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_tradeday_fullfil(all_data, is_open, company_num):\n",
    "    # 首先做一下分析，第一天数据对应的index为零到company_num-1，依次往后\n",
    "    #判断一下数据个数对不对\n",
    "    if len(all_data) == len(is_open) * company_num:\n",
    "        for index in range(len(is_open)):\n",
    "            if not is_open[index]:\n",
    "                index_list,is_deal = is_trade(index,is_open)\n",
    "                index_1 = list(map(lambda x:x*company_num,index_list))\n",
    "                index_2 = list(map(lambda x:(x+1)*company_num,index_list))\n",
    "                index_new = zip(index_1,index_2)\n",
    "                print(index,index_new)\n",
    "                # if is_deal:\n",
    "                #   all_data = news_compliment_not(index_new,all_data)\n",
    "            else:\n",
    "                #通过去两周的数据\n",
    "                index_list = range(index,index+min(15,len(is_open)-index))\n",
    "                index_1 = list(map(lambda x:x*company_num,index_list))\n",
    "                index_2 = list(map(lambda x:(x+1)*company_num,index_list))\n",
    "                index_new = zip(index_1,index_2)\n",
    "                print(index,index_new)\n",
    "                #all_data = news_compliment_yes(index_new,all_data)\n",
    "    else:\n",
    "        print('the number piece of data is wrong.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list = new_left_company(\n",
    "        'D:\\\\毕设code\\\\news_data_from2016to2021\\\\companies\\\\final_use')\n",
    "company_list = list(map(lambda x: x.replace('.csv', ''), company_list))\n",
    "news_fullfil(Year, Data_path, company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('D:\\\\毕设code\\\\news_data_from2016to2021\\\\2017\\\\2017_data.json','r') as f:\n",
    "    json_cont = json.load(f)\n",
    "for elem in json_cont:\n",
    "    for key,value in elem.items():\n",
    "        print('key',key)\n",
    "        print('value',len(value['company']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from price_preprocess import new_left_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list = new_left_company(\n",
    "        'D:\\\\毕设code\\\\news_data_from2016to2021\\\\companies\\\\final_use')\n",
    "company_list = list(map(lambda x: x.replace('.csv', ''), company_list))\n",
    "# news_fullfil(Year, Data_path, company_list)\n",
    "#第二步，文件间的补充\n",
    "data_news = new_left_company('D:\\\\毕设code\\\\data')[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from price_preprocess import get_trade_day,missing_fulfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交易日数据\n",
    "trade_day = 'D:\\\\毕设code\\\\news_data_from2016to2021\\\\companies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得我们所研究的公司列表以及代码，主要是深交所上市的2016年前上市的公司\n",
    "company_path = 'news_data_from2016to2021\\companies\\深交所A股列表_主板.xlsx'\n",
    "\n",
    "#基于tushare接口的数据。\n",
    "ts_company_path = 'news_data_from2016to2021\\companies\\\\ts_stock_price'\n",
    "\n",
    "# 各个公司的过去股价信息,需要确定具体的股票代码，网易的数据\n",
    "company_price = 'news_data_from2016to2021\\companies\\stock_price'\n",
    "\n",
    "# tushare 的登录token,需要自己注册,记得删掉自己的\n",
    "ts_entry = \"0eab484a2cff67f1cab83d7154a3e74a3accbf0f9d18a05839839dcf\"\n",
    "#储存最后使用的公司的路径\n",
    "company_final = 'D:\\毕设code\\\\news_data_from2016to2021\\companies\\\\final_use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1215, 9)\n",
      "需要补充的数据量为： 2\n",
      "trade days:1217\n",
      "company data number:1217\n",
      "(1217, 9)\n"
     ]
    }
   ],
   "source": [
    "date = get_trade_day(trade_day + '\\\\' + 'trade_cal_clean.csv')\n",
    "company_hist = pd.read_csv(os.path.join(company_final, '002137' + '.csv'),\n",
    "                           encoding='gbk').iloc[:, 2:-1]\n",
    "company_hist = company_hist[\n",
    "    company_hist['trade_date'] >= 20170101].reset_index(drop=True)\n",
    "print(company_hist.shape)\n",
    "price_updown =  missing_fulfill(date, company_hist)\n",
    "print(price_updown.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bec7dafa497d37724754715057efeea64b7cd970bf54b7a00fa6056b107842f3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('crawl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
