{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torchtext.legacy.data import Iterator\n",
    "import dill\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append('.')\n",
    "from Data import MyTextDataset, get_field, get_pre_vector\n",
    "from utils_dl import train_model, test_model\n",
    "from language_model.FastText import Model\n",
    "from utils import path\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path = path.rsplit('/', 1)[0]\n",
    "\n",
    "\n",
    "def get_parameter_number(model):\n",
    "    \"\"\"获取模型藏书量\n",
    "\n",
    "    Args:\n",
    "        model (_type_): pytorch模型\n",
    "\n",
    "    Returns:\n",
    "        ditc: 总参数量和可训练参数量的字典\n",
    "    \"\"\"\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters()\n",
    "                        if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving data path...\n",
      "reading data...\n",
      "data reading time: 1004.4600706100464\n",
      "defining fields...\n",
      "building dataset...\n",
      "receiving pretrained vectors...\n",
      "define vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab time: 769.8867354393005\n",
      "矩阵的维度为 torch.Size([445247, 300])\n",
      "矩阵的前十行为 tensor([[ 0.1696,  0.0177,  0.0050,  ...,  0.0498, -0.0477,  0.2002],\n",
      "        [ 0.0625,  0.0278,  0.0178,  ..., -0.0122, -0.0442, -0.0020],\n",
      "        [ 0.0786, -0.0027, -0.1833,  ...,  0.1627, -0.1661,  0.0388],\n",
      "        ...,\n",
      "        [ 0.0633, -0.1805, -0.0417,  ...,  0.0109, -0.1885,  0.2579],\n",
      "        [-0.1469,  0.2031, -0.0347,  ...,  0.2687, -0.3711,  0.0732],\n",
      "        [ 0.2368, -0.0158,  0.2339,  ..., -0.2937, -0.1052,  0.0433]])\n",
      "define iterators...\n",
      "define model parameter...\n",
      "define model...\n",
      "模型结构:\n",
      "Model(\n",
      "  (embedding): Embedding(445247, 300)\n",
      "  (fc): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "模型总参数量为:\n",
      "total:133741970,size:133.74197M\n",
      "模型可训练参数量为:\n",
      "train:133741970,size:133.74197M\n",
      "define optimizer and criterion...\n",
      "begin training...\n"
     ]
    }
   ],
   "source": [
    "#获取数据集路径\n",
    "print('receiving data path...')\n",
    "train_data = os.path.join(path, 'data', 'train_set')\n",
    "validation_data = os.path.join(path, 'data', 'validation_set')\n",
    "test_data = os.path.join(path, 'data', 'test_set')\n",
    "#读取数据\n",
    "print('reading data...')\n",
    "start_time = time.time()\n",
    "with open(train_data, 'rb') as f:\n",
    "    train_example = dill.load(f)\n",
    "with open(validation_data, 'rb') as f:\n",
    "    validation_example = dill.load(f)\n",
    "with open(test_data, 'rb') as f:\n",
    "    test_example = dill.load(f)\n",
    "end_time = time.time()\n",
    "print('data reading time:', end_time - start_time)\n",
    "#定义字段\n",
    "print('defining fields...')\n",
    "NEWS, LAST_NEWS_PERIOD, LABEL = get_field()\n",
    "fields = [('news', NEWS), ('news_period', LAST_NEWS_PERIOD),\n",
    "          (\"label\", LABEL)]\n",
    "#构建数据,不单独区分测试集了\n",
    "print('building dataset...')\n",
    "train_set = MyTextDataset(examples=train_example + validation_example,\n",
    "                          fields=fields)\n",
    "test_set = MyTextDataset(examples=test_example, fields=fields)\n",
    "#获取预训练词向量\n",
    "print('receiving pretrained vectors...')\n",
    "vectors = get_pre_vector() #注Vectors源文件做了修改，见https://blog.csdn.net/qq_23262411/article/details/100173224\n",
    "#定义词表\n",
    "print('define vocab...')\n",
    "time3 = time.time()\n",
    "NEWS.build_vocab(train_set, test_set, vectors=vectors,\n",
    "                 min_freq=10)  #去除频率小于十次的词\n",
    "#NEWS.vocab.vectors.unk_init = init.xavier_uniform_\n",
    "weight_vector = NEWS.vocab.vectors\n",
    "time4 = time.time()\n",
    "print('building vocab time:', time4 - time3)\n",
    "print('矩阵的维度为', weight_vector.shape)\n",
    "print('矩阵的前十行为', weight_vector[:10])\n",
    "#生成迭代器\n",
    "print('define iterators...')\n",
    "train_iter = Iterator(train_set,\n",
    "                     batch_size=4096,\n",
    "                     device=-1,\n",
    "                     sort_key=lambda x: len(x.news),\n",
    "                     sort_within_batch=False,\n",
    "                     repeat=False)\n",
    "test_iter = Iterator(test_set,\n",
    "                    batch_size=4096,\n",
    "                    device=-1,\n",
    "                    sort=False,\n",
    "                    sort_within_batch=False,\n",
    "                    repeat=False)\n",
    "#定义模型参数\n",
    "print('define model parameter...')\n",
    "n_vocab = len(NEWS.vocab)\n",
    "embed_dims = 300\n",
    "hidden_size = 256\n",
    "num_classes = 2\n",
    "weight_matrix = weight_vector\n",
    "model_name = 'FastText'\n",
    "n_epochs = 1 #先小点做测试\n",
    "lr_ = 0.02\n",
    "weight_decay_ = 0.0002\n",
    "model_path = '/new_python_for_gnn/毕设code/model_cache'\n",
    "#模型实例化\n",
    "print('define model...')\n",
    "fasttext = Model(n_vocab=n_vocab,\n",
    "                 embed_dims=embed_dims,\n",
    "                 hidden_size=hidden_size,\n",
    "                 num_classes=num_classes,\n",
    "                 weight_matrix=weight_matrix)\n",
    "print('模型结构:')\n",
    "print(fasttext)\n",
    "parameters_num = get_parameter_number(fasttext)\n",
    "print('模型总参数量为:')\n",
    "print('total:{},size:{}M'.format(parameters_num['Total'],\n",
    "                                 parameters_num['Total'] / 1e6))\n",
    "print('模型可训练参数量为:')\n",
    "print('train:{},size:{}M'.format(parameters_num['Trainable'],\n",
    "                                 parameters_num['Trainable'] / 1e6))\n",
    "#定义训练部件\n",
    "print('define optimizer and criterion...')\n",
    "optimizer = optim.Adam(fasttext.parameters(), lr=lr_, weight_decay=weight_decay_)\n",
    "criterion = F.cross_entropy\n",
    "#开始训练\n",
    "print('begin training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            =======  Training  ======= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "total_step = len(train_iter)\n",
    "print(\"            =======  Training  ======= \\n\")\n",
    "train_loss_ = []  #一个epoch的训练误差\n",
    "correct_ = []  #一个epoch的精度\n",
    "accuracy_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "train_loss += 12.5\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [  1/1], Step: [ 25/267], Ave_Loss: 10.178, acc: 50.236\n",
      "Epoch: [  1/1], Step: [ 50/267], Ave_Loss: 7.716, acc: 50.882\n",
      "Epoch: [  1/1], Step: [ 75/267], Ave_Loss: 5.411, acc: 51.458\n",
      "Epoch: [  1/1], Step: [100/267], Ave_Loss: 4.234, acc: 51.755\n",
      "Epoch: [  1/1], Step: [125/267], Ave_Loss: 3.553, acc: 51.911\n",
      "Epoch: [  1/1], Step: [150/267], Ave_Loss: 3.094, acc: 52.066\n",
      "Epoch: [  1/1], Step: [175/267], Ave_Loss: 2.752, acc: 52.148\n",
      "Epoch: [  1/1], Step: [200/267], Ave_Loss: 2.494, acc: 52.208\n",
      "Epoch: [  1/1], Step: [225/267], Ave_Loss: 2.294, acc: 52.260\n",
      "Epoch: [  1/1], Step: [250/267], Ave_Loss: 2.134, acc: 52.269\n",
      "Epoch: [  1/1], Step: [267/267], Ave_Loss: 2.042, acc: 52.294\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        data, labels = batch.news, batch.label\n",
    "        period = batch.news_period\n",
    "        #print(period)\n",
    "        data = data.cuda()\n",
    "        labels = labels.cuda().long()\n",
    "        period = period.cuda()\n",
    "        #print(data.shape) [1024,1000]\n",
    "        #print(labels.shape) [1024]\n",
    "        #print(period.shape) [1024]\n",
    "        # Forward\n",
    "        outputs = model(data, period)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #train_loss += loss.item()\n",
    "        train_loss += float(loss)\n",
    "        total += labels.size(0)\n",
    "        correct += torch.eq(outputs.argmax(dim=1), labels).sum().item()\n",
    "        accuracy = 100.0 * correct / total\n",
    "        if (i + 1) % 25 == 0 or (i + 1) == total_step:\n",
    "            print(\n",
    "                'Epoch: [{:3}/{}], Step: [{:3}/{}], Ave_Loss: {:.3f}, acc: {:6.3f}'\n",
    "                .format(\n",
    "                    epoch + 1,\n",
    "                    n_epochs,\n",
    "                    i + 1,\n",
    "                    total_step,\n",
    "                    train_loss / (i + 1),\n",
    "                    accuracy,\n",
    "                ))\n",
    "    train_loss_.append(train_loss)\n",
    "    correct_.append(correct)\n",
    "    accuracy_.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.311806678772"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            =======  Training Finished  ======= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n            =======  Training Finished  ======= \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = os.path.join('/new_python_for_gnn/毕设code/model_cache', model_name + '.pt')\n",
    "if not os.path.exists(PATH):\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "print(\"\\nModel saved... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            =======  Testing  ======= \n",
      "\n",
      "test accuracy 0.538, test macro f1-score 0.350\n",
      "\n",
      "            =======  Testing Finished  ======= \n",
      "\n",
      "\n",
      "  calculating classification index...  \n",
      "\n",
      "test_result的混淆矩阵为:\n",
      "\n",
      "[[     0 125162]\n",
      " [     0 145636]]\n",
      "准确率为0.5378030856948722\n",
      "精确率为0.5378030856948722\n",
      "召回率为1.0\n",
      "特异度为0.0\n",
      "假报警率为1.0\n",
      "真正率为1.0\n",
      "f1分数为0.6994433691773486\n",
      "g_mean值为0.0\n",
      "成功保存测试结果！\n",
      "\n",
      "  finished saving index file...  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils_dl import test_model\n",
    "test_model(fasttext,'/new_python_for_gnn/毕设code/model_cache/FastText.pt',test_iter,'Fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torchtext.legacy.data import Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import dill\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from Data import MyTextDataset, get_field, get_pre_vector, PriceDataset\n",
    "from utils_dl import train_model, test_model\n",
    "from language_model.FastText import Model\n",
    "from utils import path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "path = path.rsplit('/', 1)[0]\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters()\n",
    "                        if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving data path...\n",
      "reading data...\n",
      "data reading time: 1046.9419722557068\n"
     ]
    }
   ],
   "source": [
    "print('receiving data path...')\n",
    "train_data = os.path.join(path, 'data', 'train_set')\n",
    "validation_data = os.path.join(path, 'data', 'validation_set')\n",
    "test_data = os.path.join(path, 'data', 'test_set')\n",
    "#读取数据\n",
    "print('reading data...')\n",
    "start_time = time.time()\n",
    "with open(train_data, 'rb') as f:\n",
    "    train_example = dill.load(f)\n",
    "with open(validation_data, 'rb') as f:\n",
    "    validation_example = dill.load(f)\n",
    "with open(test_data, 'rb') as f:\n",
    "    test_example = dill.load(f)\n",
    "end_time = time.time()\n",
    "print('data reading time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining fields...\n",
      "building dataset...\n",
      "receiving pretrained vectors...\n"
     ]
    }
   ],
   "source": [
    "print('defining fields...')\n",
    "NEWS, LAST_NEWS_PERIOD, LABEL = get_field()\n",
    "fields = [('news', NEWS), ('news_period', LAST_NEWS_PERIOD),\n",
    "          (\"label\", LABEL)]\n",
    "#构建数据,不单独区分测试集了\n",
    "print('building dataset...')\n",
    "train_set = MyTextDataset(examples=train_example + validation_example,\n",
    "                          fields=fields)\n",
    "test_set = MyTextDataset(examples=test_example, fields=fields)\n",
    "#获取预训练词向量\n",
    "print('receiving pretrained vectors...')\n",
    "vectors = get_pre_vector()  #注Vectors源文件做了修改，见https://blog.csdn.net/qq_23262411/article/details/100173224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define vocab...\n",
      "building vocab time: 749.8627281188965\n",
      "矩阵的维度为 torch.Size([445247, 300])\n"
     ]
    }
   ],
   "source": [
    "#定义词表\n",
    "print('define vocab...')\n",
    "time3 = time.time()\n",
    "NEWS.build_vocab(train_set, test_set, vectors=vectors,\n",
    "                 min_freq=10)  #去除频率小于十次的词\n",
    "#NEWS.vocab.vectors.unk_init = init.xavier_uniform_\n",
    "weight_vector = NEWS.vocab.vectors\n",
    "time4 = time.time()\n",
    "print('building vocab time:', time4 - time3)\n",
    "print('矩阵的维度为', weight_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define iterators...\n"
     ]
    }
   ],
   "source": [
    "print('define iterators...')\n",
    "train_iter = Iterator(train_set,\n",
    "                      batch_size=4096,\n",
    "                      device=-1,\n",
    "                      sort_key=lambda x: len(x.news),\n",
    "                      sort_within_batch=False,\n",
    "                      shuffle=False,\n",
    "                      repeat=False)\n",
    "test_iter = Iterator(test_set,\n",
    "                     batch_size=4096,\n",
    "                     device=-1,\n",
    "                     sort=False,\n",
    "                     sort_within_batch=False,\n",
    "                     shuffle=False,\n",
    "                     repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price dataset...\n",
      "price iterators...\n"
     ]
    }
   ],
   "source": [
    "print('price dataset...')\n",
    "train_price = PriceDataset(train_or_test='train')\n",
    "test_price = PriceDataset(train_or_test='test')\n",
    "#构建迭代器\n",
    "print('price iterators...')\n",
    "train_price_iter = DataLoader(train_price,\n",
    "                              batch_size=4096,\n",
    "                              shuffle=False)\n",
    "test_price_iter = DataLoader(test_price,\n",
    "                             batch_size=4096,\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define model parameter...\n"
     ]
    }
   ],
   "source": [
    "print('define model parameter...')\n",
    "n_vocab = len(NEWS.vocab)\n",
    "embed_dims = 300\n",
    "hidden_size = 256\n",
    "num_classes = 2\n",
    "weight_matrix = weight_vector\n",
    "model_name = 'FastText'\n",
    "n_epochs = 1  #先小点做测试\n",
    "lr_ = 0.02\n",
    "weight_decay_ = 0.0002\n",
    "model_path = '/new_python_for_gnn/毕设code/model_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define model...\n",
      "模型结构:\n",
      "Model(\n",
      "  (embedding): Embedding(445247, 300)\n",
      "  (fc): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=304, out_features=256, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "模型总参数量为:\n",
      "total:133742994,size:133.742994M\n",
      "模型可训练参数量为:\n",
      "train:133742994,size:133.742994M\n",
      "define optimizer and criterion...\n"
     ]
    }
   ],
   "source": [
    "print('define model...')\n",
    "fasttext = Model(n_vocab=n_vocab,\n",
    "                 embed_dims=embed_dims,\n",
    "                 hidden_size=hidden_size,\n",
    "                 num_classes=num_classes,\n",
    "                 weight_matrix=weight_matrix)\n",
    "print('模型结构:')\n",
    "print(fasttext)\n",
    "parameters_num = get_parameter_number(fasttext)\n",
    "print('模型总参数量为:')\n",
    "print('total:{},size:{}M'.format(parameters_num['Total'],\n",
    "                                 parameters_num['Total'] / 1e6))\n",
    "print('模型可训练参数量为:')\n",
    "print('train:{},size:{}M'.format(parameters_num['Trainable'],\n",
    "                                 parameters_num['Trainable'] / 1e6))\n",
    "#定义训练部件\n",
    "print('define optimizer and criterion...')\n",
    "optimizer = optim.Adam(fasttext.parameters(),\n",
    "                       lr=lr_,\n",
    "                       weight_decay=weight_decay_)\n",
    "criterion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            =======  Training  ======= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "total_step = len(train_iter)\n",
    "print(\"            =======  Training  ======= \\n\")\n",
    "train_loss_ = []  #一个epoch的训练误差\n",
    "correct_ = []  #一个epoch的精度\n",
    "accuracy_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [  1/1], Step: [ 25/267], Ave_Loss: 0.806, acc: 50.895\n",
      "Epoch: [  1/1], Step: [ 50/267], Ave_Loss: 0.811, acc: 51.593\n",
      "Epoch: [  1/1], Step: [ 75/267], Ave_Loss: 0.772, acc: 52.173\n",
      "Epoch: [  1/1], Step: [100/267], Ave_Loss: 0.753, acc: 52.275\n",
      "Epoch: [  1/1], Step: [125/267], Ave_Loss: 0.744, acc: 52.159\n",
      "Epoch: [  1/1], Step: [150/267], Ave_Loss: 0.736, acc: 51.316\n",
      "Epoch: [  1/1], Step: [175/267], Ave_Loss: 0.743, acc: 51.691\n",
      "Epoch: [  1/1], Step: [200/267], Ave_Loss: 0.737, acc: 51.369\n",
      "Epoch: [  1/1], Step: [225/267], Ave_Loss: 0.732, acc: 51.217\n",
      "Epoch: [  1/1], Step: [250/267], Ave_Loss: 0.728, acc: 51.720\n",
      "Epoch: [  1/1], Step: [267/267], Ave_Loss: 0.726, acc: 51.823\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (batch,price) in enumerate(zip(train_iter,train_price_iter)):\n",
    "        data, labels = batch.news, batch.label\n",
    "        period = batch.news_period\n",
    "        #print(period)\n",
    "        data = data.cuda()\n",
    "        labels = labels.cuda().long()\n",
    "        period = period.cuda()\n",
    "        price = price.cuda().float()\n",
    "        #print(data.shape) [1024,1000]\n",
    "        #print(labels.shape) [1024]\n",
    "        #print(period.shape) [1024]\n",
    "        # Forward\n",
    "        outputs = model(data, period,price)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #train_loss += loss.item()\n",
    "        train_loss += float(loss)\n",
    "        total += labels.size(0)\n",
    "        correct += torch.eq(outputs.argmax(dim=1), labels).sum().item()\n",
    "        accuracy = 100.0 * correct / total\n",
    "        if (i + 1) % 25 == 0 or (i + 1) == total_step:\n",
    "            print(\n",
    "                'Epoch: [{:3}/{}], Step: [{:3}/{}], Ave_Loss: {:.3f}, acc: {:6.3f}'\n",
    "                .format(\n",
    "                    epoch + 1,\n",
    "                    n_epochs,\n",
    "                    i + 1,\n",
    "                    total_step,\n",
    "                    train_loss / (i + 1),\n",
    "                    accuracy,\n",
    "                ))\n",
    "    train_loss_.append(train_loss)\n",
    "    correct_.append(correct)\n",
    "    accuracy_.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " loss saving... \n",
      "\n",
      "\n",
      " success for loss saving...\n",
      "\n",
      "\n",
      "Model saving... \n",
      "\n",
      "\n",
      "Model saved... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "print('\\n loss saving... \\n')\n",
    "with open(\n",
    "        os.path.join( '/new_python_for_gnn/毕设code/language_model_result/train',\n",
    "                     model_name + '_loss.json'), 'wb') as f:\n",
    "    loss_file = OrderedDict()\n",
    "    loss_file['train_loss'] = train_loss_\n",
    "    loss_file['correct'] = correct_\n",
    "    loss_file['accuracy'] = accuracy_\n",
    "    json_cont = json.dumps(loss_file, indent=4).encode(encoding='utf-8')\n",
    "    f.write(json_cont)\n",
    "print('\\n success for loss saving...\\n')\n",
    "#保存模型方便之后进行模型测试等操作\n",
    "print(\"\\nModel saving... \\n\")\n",
    "PATH = os.path.join( '/new_python_for_gnn/毕设code/model_cache', model_name + '.pt')\n",
    "if not os.path.exists(PATH):\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "print(\"\\nModel saved... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training...\n",
      "            =======  Training  ======= \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/new_python_for_gnn/毕设code/language_train/../language_model/FastText.py\", line 51, in forward\n    out = self.fc1(out)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 96, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\", line 1847, in linear\n    return torch._C._nn.linear(input, weight, bias)\nRuntimeError: expected scalar type Float but found Double\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16506/1963236630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtime5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_model(fasttext, train_iter, train_price_iter, optimizer, criterion,\n\u001b[0m\u001b[1;32m      4\u001b[0m             n_epochs, model_name)\n\u001b[1;32m      5\u001b[0m \u001b[0mtime6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/new_python_for_gnn/毕设code/language_train/utils_dl.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_iter, train_price_iter, optimizer, criterion, n_epochs, model_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m#print(period.shape) [1024]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/new_python_for_gnn/毕设code/language_train/../language_model/FastText.py\", line 51, in forward\n    out = self.fc1(out)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 96, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\", line 1847, in linear\n    return torch._C._nn.linear(input, weight, bias)\nRuntimeError: expected scalar type Float but found Double\n"
     ]
    }
   ],
   "source": [
    "print('begin training...')\n",
    "time5 = time.time()\n",
    "train_model(fasttext, train_iter, train_price_iter, optimizer, criterion,\n",
    "            n_epochs, model_name)\n",
    "time6 = time.time()\n",
    "print('training time:', time6 - time5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin tesing...\n",
      "\n",
      "            =======  Testing  ======= \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/new_python_for_gnn/毕设code/language_train/../language_model/FastText.py\", line 51, in forward\n    out = self.fc1(out)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 96, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\", line 1847, in linear\n    return torch._C._nn.linear(input, weight, bias)\nRuntimeError: expected scalar type Float but found Double\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16506/2597157941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin tesing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtime7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m test_model(fasttext, os.path.join(model_path, model_name + '.pt'),\n\u001b[0m\u001b[1;32m      4\u001b[0m            test_iter, test_price_iter, model_name)\n\u001b[1;32m      5\u001b[0m \u001b[0mtime8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/new_python_for_gnn/毕设code/language_train/utils_dl.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, model_path, test_iter, test_price_iter, model_name)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mbatch_zs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_zs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             batch_out = model(batch_xs, batch_zs,\n\u001b[1;32m    150\u001b[0m                               price)  # (batch_size, num_classes)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/new_python_for_gnn/毕设code/language_train/../language_model/FastText.py\", line 51, in forward\n    out = self.fc1(out)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 96, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/nsc/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\", line 1847, in linear\n    return torch._C._nn.linear(input, weight, bias)\nRuntimeError: expected scalar type Float but found Double\n"
     ]
    }
   ],
   "source": [
    "print('begin tesing...')\n",
    "time7 = time.time()\n",
    "test_model(fasttext, os.path.join(model_path, model_name + '.pt'),\n",
    "           test_iter, test_price_iter, model_name)\n",
    "time8 = time.time()\n",
    "print('testing time:', time8 - time7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torchtext.legacy.data import Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import dill\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append('.')\n",
    "from Data import MyTextDataset, get_field, get_pre_vector, PriceDataset\n",
    "from utils_dl import train_model, test_model, get_parameter_number, company_deal\n",
    "from language_model.FastText import Model\n",
    "from utils import path\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path = path.rsplit('/', 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving data path...\n",
      "reading data...\n",
      "data reading time: 1046.096774816513\n"
     ]
    }
   ],
   "source": [
    "print('receiving data path...')\n",
    "train_data = os.path.join(path, 'data', 'train_set')\n",
    "validation_data = os.path.join(path, 'data', 'validation_set')\n",
    "test_data = os.path.join(path, 'data', 'test_set')\n",
    "#读取数据\n",
    "print('reading data...')\n",
    "start_time = time.time()\n",
    "with open(train_data, 'rb') as f:\n",
    "    train_example = dill.load(f)\n",
    "with open(validation_data, 'rb') as f:\n",
    "    validation_example = dill.load(f)\n",
    "with open(test_data, 'rb') as f:\n",
    "    test_example = dill.load(f)\n",
    "end_time = time.time()\n",
    "print('data reading time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_example[12].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS, LAST_NEWS_PERIOD, LABEL = get_field()\n",
    "fields = [('news', NEWS), ('news_period', LAST_NEWS_PERIOD),\n",
    "              (\"label\", LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyTextDataset(examples=train_example + validation_example,\n",
    "                              fields=fields)\n",
    "test_set = MyTextDataset(examples=test_example, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection...\n",
      "train_set...\n"
     ]
    }
   ],
   "source": [
    "print('feature selection...')\n",
    "time9 = time.time()\n",
    "train_set, test_set = company_deal(train_set, test_set)\n",
    "time10 = time.time()\n",
    "print('feature selection time:', time10 - time9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('receiving pretrained vectors...')\n",
    "vectors = get_pre_vector() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time3 = time.time()\n",
    "NEWS.build_vocab(\n",
    "    train_set, test_set, vectors=vectors\n",
    ")  #去除频率小于十次的词,这会导致一部分资讯没有内容，因此后来不保留这个min_fre(最一开始有的），也能留下比较好的特征\n",
    "#NEWS.vocab.vectors.unk_init = init.xavier_uniform_\n",
    "weight_vector = NEWS.vocab.vectors\n",
    "time4 = time.time()\n",
    "print('building vocab time:', time4 - time3)\n",
    "print('矩阵的维度为', weight_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('define iterators...')\n",
    "BATCH_SIZE = 4096\n",
    "train_iter = Iterator(train_set,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      device=-1,\n",
    "                      sort_key=lambda x: len(x.news),\n",
    "                      sort_within_batch=False,\n",
    "                      shuffle=False,\n",
    "                      repeat=False)\n",
    "test_iter = Iterator(test_set,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     device=-1,\n",
    "                     sort=False,\n",
    "                     sort_within_batch=False,\n",
    "                     shuffle=False,\n",
    "                     repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('price dataset...')\n",
    "train_price = PriceDataset(train_or_test='train')\n",
    "test_price = PriceDataset(train_or_test='test')\n",
    "#构建迭代器\n",
    "print('price iterators...')\n",
    "train_price_iter = DataLoader(train_price, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_price_iter = DataLoader(test_price, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define model parameter...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NEWS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29811/3582292147.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'define model parameter...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNEWS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membed_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NEWS' is not defined"
     ]
    }
   ],
   "source": [
    "print('define model parameter...')\n",
    "n_vocab = len(NEWS.vocab)\n",
    "embed_dims = 300\n",
    "hidden_size = 256\n",
    "num_classes = 2\n",
    "weight_matrix = weight_vector\n",
    "model_name = 'FastText'\n",
    "n_epochs = 30  #先小点做测试\n",
    "lr_ = 0.03\n",
    "weight_decay_ = 0.0003\n",
    "model_path = '/new_python_for_gnn/毕设code/model_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('define model...')\n",
    "fasttext = Model(n_vocab=n_vocab,\n",
    "                 embed_dims=embed_dims,\n",
    "                 hidden_size=hidden_size,\n",
    "                 num_classes=num_classes,\n",
    "                 weight_matrix=weight_matrix)\n",
    "print('模型结构:')\n",
    "print(fasttext)\n",
    "parameters_num = get_parameter_number(fasttext)\n",
    "print('模型总参数量为:')\n",
    "print('total:{},size:{}M'.format(parameters_num['Total'],\n",
    "                                 parameters_num['Total'] / 1024**2))\n",
    "print('模型可训练参数量为:')\n",
    "print('train:{},size:{}M'.format(parameters_num['Trainable'],\n",
    "                                 parameters_num['Trainable'] / 1024**2))\n",
    "#定义训练部件\n",
    "print('define optimizer and criterion...')\n",
    "optimizer = optim.Adam(fasttext.parameters(),\n",
    "                       lr=lr_,\n",
    "                       weight_decay=weight_decay_)\n",
    "criterion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('begin training...')\n",
    "time5 = time.time()\n",
    "train_model(fasttext, train_iter, train_price_iter, optimizer, criterion,\n",
    "            n_epochs, model_name)\n",
    "time6 = time.time()\n",
    "print('training time:', time6 - time5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('begin tesing...')\n",
    "time7 = time.time()\n",
    "test_model(fasttext, os.path.join(model_path, model_name + '.pt'),\n",
    "           test_iter, test_price_iter, model_name)\n",
    "time8 = time.time()\n",
    "print('testing time:', time8 - time7)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a858e51b3809aed9a5f06a8465667219137969dbd429606ccb60b48ceaeaac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
